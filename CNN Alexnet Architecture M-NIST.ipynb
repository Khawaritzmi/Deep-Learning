{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alexnet(Nguli).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khawaritzmi/Deep-Learning/blob/master/CNN%20Alexnet%20Architecture%20M-NIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxWX-Hpphe_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evals :  Model evaluation's function.\n",
        "import tensorflow as f\n",
        "\n",
        "\n",
        "def calc_loss_acc(labels, logits):\n",
        "    \"\"\"Function to compute loss value. Here, we used cross entropy \n",
        "    Args:\n",
        "        logits: 4D tensor. output tensor from segnet model, which is the output of softmax\n",
        "        labels: true labels tensor\n",
        "    Returns:\n",
        "        loss (cross_entropy_mean), accuracy, predicts(logits with softmax) \n",
        "    \"\"\"\n",
        "    # calc cross entropy mean  cross_entropy\n",
        "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='cross_entropy')\n",
        "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
        "    tf.summary.scalar(name='loss', tensor=cross_entropy_mean)\n",
        "\n",
        "\n",
        "    predicts = tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1))\n",
        "\n",
        "    accuracy = tf.reduce_mean(tf.cast(predicts, dtype=tf.float32))\n",
        "    tf.summary.scalar(name='accuracy', tensor=accuracy)\n",
        "\n",
        "    return cross_entropy_mean, accuracy, predicts\n",
        "\n",
        "\n",
        "def train_op(total_loss, global_steps, base_learning_rate, option='Adam'):\n",
        "    \"\"\"This function defines train optimizer \n",
        "    Args:\n",
        "        total_loss: the loss value\n",
        "        global_steps: global steps is used to track how many batch had been passed. In the training process, the initial value for global_steps = 0, here  \n",
        "        global_steps=tf.Variable(0, trainable=False). then after one batch of images passed, the loss is passed into the optimizer to update the weight, then the global \n",
        "        step increased by one.\n",
        "        base_learning_rate: default value 0.1\n",
        "    Returns:\n",
        "        the train optimizer\n",
        "    \"\"\"\n",
        "\n",
        "    # base_learning_rate = 0.01\n",
        "    # get update operation\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "\n",
        "        if option == 'Adam':\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=base_learning_rate)\n",
        "            print(\"Running with Adam Optimizer with learning rate:\", base_learning_rate)\n",
        "        elif option == 'SGD':\n",
        "            # base_learning_rate = 0.01\n",
        "            learning_rate_decay = tf.train.exponential_decay(base_learning_rate, global_steps, 1000, 0.0005)\n",
        "            optimizer = tf.train.GradientDescentOptimizer(learning_rate_decay)\n",
        "            print(\"Running with SGD Optimizer with learning rate:\", learning_rate_decay)\n",
        "        else:\n",
        "            raise ValueError('Optimizer is not recognized')\n",
        "\n",
        "        grads = optimizer.compute_gradients(total_loss, var_list=tf.trainable_variables())\n",
        "        training_op = optimizer.apply_gradients(grads, global_step=global_steps)\n",
        "\n",
        "    return training_op\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS3dJ1fphHeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Layers : Neuron network layers containing convolutional layer, full collection layer, normalization and maxpooling.\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def max_pooling(inputs, ksize, stride, padding, name):\n",
        "    \"\"\"Create max pooling layer \n",
        "    Args:\n",
        "        inputs: float32 4D tensor\n",
        "        ksize: a tuple of 2 int with (kernel_height, kernel_width)\n",
        "        stride: a tuple\n",
        "        padding: string. padding mode 'SAME', '\n",
        "        name: string\n",
        "        \n",
        "    Returns:\n",
        "        4D tensor of [batch_size, height, width, channels]\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        value = tf.nn.max_pool(inputs, ksize=[1, ksize[0], ksize[1], 1], strides=[1, stride[0], stride[1], 1], padding=padding, name=scope.name)\n",
        "    return value\n",
        "\n",
        "def dropout(inputs, keep_prob, name):\n",
        "    \"\"\"Dropout layer\n",
        "    Args:\n",
        "        inputs: float32 4D tensor \n",
        "        keep_prob: the probability of keep training sample\n",
        "        name: layer name to define\n",
        "    Returns:\n",
        "        4D tensor of [batch_size, height, width, channels]\n",
        "    \"\"\"\n",
        "    \n",
        "    return tf.nn.dropout(inputs, rate=(1-keep_prob), name=name)\n",
        "\n",
        "\n",
        "def norm(inputs, radius=4, name=None):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        value = tf.nn.lrn(inputs, depth_radius=radius, bias=1.0, alpha=1e-4, beta=0.75, name=name)\n",
        "    return value\n",
        "\n",
        "# def batch_norm(inputs, name):\n",
        "#     \"\"\"batch normalization layer\n",
        "\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "def conv2d(inputs, shape, padding, name):\n",
        "    \"\"\"Create convolution 2D layer\n",
        "    Args:\n",
        "        inputs: float32. 4D tensor\n",
        "        shape: the shape of kernel \n",
        "        padding: string. padding mode 'SAME',\n",
        "        name: corressponding layer's name\n",
        "    Returns:\n",
        "        Output 4D tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        # get weights value and record a summary protocol buffer with a histogram\n",
        "        weights = tf.get_variable('weights', shape=shape, initializer=tf.initializers.he_normal())\n",
        "        tf.summary.histogram(scope.name + 'weights', weights)\n",
        "\n",
        "        # get biases value and record a summary protocol buffer with a histogram\n",
        "        biases = tf.get_variable('biases', shape=shape[3], initializer=tf.initializers.random_normal())\n",
        "        tf.summary.histogram(scope.name + 'biases', biases)\n",
        "        # compute convlotion W * X + b, activiation function relu function\n",
        "        outputs = tf.nn.conv2d(inputs, weights, strides=[1, 1, 1, 1], padding=padding)\n",
        "        outputs = tf.nn.bias_add(outputs, biases)\n",
        "        outputs = tf.nn.relu(outputs, name=scope.name + 'relu')\n",
        "    return outputs\n",
        "\n",
        "def fc(inputs, shape, name):\n",
        "    \"\"\"Create fully collection layer \n",
        "    Args:\n",
        "        inputs: Float32. 2D tensor with shape [batch, input_units]\n",
        "        shape: Int. a tuple with [num_inputs, num_outputs]\n",
        "        name: sring. layer name\n",
        "    Returns:\n",
        "        Outputs fully collection tensor\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        weights = tf.get_variable('weights', shape = [shape[0], shape[1]], initializer=tf.initializers.he_normal())\n",
        "        biases = tf.get_variable('biases', shape = [shape[1]], initializer=tf.initializers.random_normal())\n",
        "        # outputs = tf.nn.xw_plus_b(inputs, weights, biases, name = scope.name)\n",
        "        outputs = tf.add(tf.matmul(inputs, weights), biases, name=scope.name)\n",
        "\n",
        "    return tf.nn.relu(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppf5kauqhAO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model : Class with the graph definition of the AlexNet.\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def alexnet(inputs, num_classes, keep_prob):\n",
        "    \"\"\"Create alexnet model\n",
        "    \"\"\"\n",
        "    x = tf.reshape(inputs, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # first conv layer, downsampling layer, and normalization layer\n",
        "    conv1 = conv2d(x, shape=(11, 11, 1, 96), padding='SAME', name='conv1')\n",
        "    pool1 = max_pooling(conv1, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool1')\n",
        "    norm1 = norm(pool1, radius=4, name='norm1')\n",
        "\n",
        "    # second conv layer\n",
        "    conv2 = conv2d(norm1, shape=(5, 5, 96, 256), padding='SAME', name='conv2')\n",
        "    pool2 = max_pooling(conv2, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool2')\n",
        "    norm2 = norm(pool2, radius=4, name='norm2')\n",
        "\n",
        "    # 3rd conv layer\n",
        "    conv3 = conv2d(norm2, shape=(3, 3, 256, 384), padding='SAME', name='conv3')\n",
        "    # pool3 = max_pooling(conv3, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool3')\n",
        "    norm3 = norm(conv3, radius=4, name='norm3')\n",
        "\n",
        "    # 4th conv layer\n",
        "    conv4 = conv2d(norm3, shape=(3, 3, 384, 384), padding='SAME', name='conv4')\n",
        "\n",
        "    # 5th conv layer\n",
        "    conv5 = conv2d(conv4, shape=(3, 3, 384, 256), padding='SAME', name='conv5')\n",
        "    pool5 = max_pooling(conv5, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool5')\n",
        "    norm5 = norm(pool5, radius=4, name='norm5')\n",
        "\n",
        "    # first fully connected layer\n",
        "    fc1 = tf.reshape(norm5, shape=(-1, 4*4*256))\n",
        "    fc1 = fc(fc1, shape=(4*4*256, 4096), name='fc1')\n",
        "    fc1 = dropout(fc1, keep_prob=keep_prob, name='dropout1')\n",
        "\n",
        "    fc2 = fc(fc1, shape=(4096, 4096), name='fc2')\n",
        "    fc2 = dropout(fc2, keep_prob=keep_prob, name='dropout2')\n",
        "\n",
        "    # output logits value\n",
        "    with tf.variable_scope('classifier') as scope:\n",
        "        weights = tf.get_variable('weights', shape=[4096, num_classes], initializer=tf.initializers.he_normal())\n",
        "        biases = tf.get_variable('biases', shape=[num_classes], initializer=tf.initializers.random_normal())\n",
        "        # define output logits value\n",
        "        logits = tf.add(tf.matmul(fc2, weights), biases, name=scope.name + '_logits')\n",
        "\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6RLWYm1RcGa",
        "colab_type": "code",
        "outputId": "aa22a5d3-5365-45e4-b493-179c65cfc957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Train\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "flags.DEFINE_integer('valid_steps', 11, 'The number of validation steps ')\n",
        "\n",
        "flags.DEFINE_integer('max_steps', 1001, 'The number of maximum steps for traing')\n",
        "\n",
        "flags.DEFINE_integer('batch_size', 128, 'The number of images in each batch during training')\n",
        "\n",
        "flags.DEFINE_float('base_learning_rate', 0.0001, \"base learning rate for optimizer\")\n",
        "\n",
        "flags.DEFINE_integer('input_shape', 784, 'The inputs tensor shape')\n",
        "\n",
        "flags.DEFINE_integer('num_classes', 10, 'The number of label classes')\n",
        "\n",
        "flags.DEFINE_string('save_dir', './outputs', 'The path to saved checkpoints')\n",
        "\n",
        "flags.DEFINE_float('keep_prob', 0.75, \"the probability of keeping neuron unit\")\n",
        "\n",
        "flags.DEFINE_string('tb_path', './tb_logs/', 'The path points to tensorboard logs ')\n",
        "\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "\n",
        "def train(FLAGS):\n",
        "    \"\"\"Training model\n",
        "    \"\"\"\n",
        "    valid_steps = FLAGS.valid_steps\n",
        "    max_steps = FLAGS.max_steps\n",
        "    batch_size = FLAGS.batch_size\n",
        "    base_learning_rate = FLAGS.base_learning_rate\n",
        "    input_shape = FLAGS.input_shape  # image shape = 28 * 28\n",
        "    num_classes = FLAGS.num_classes\n",
        "    keep_prob = FLAGS.keep_prob\n",
        "    save_dir = FLAGS.save_dir\n",
        "    tb_path = FLAGS.tb_path \n",
        "\n",
        "    train_loss, train_acc = [], []\n",
        "    valid_loss, valid_acc = [], []\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    # define default tensor graphe \n",
        "    with tf.Graph().as_default():\n",
        "        images_pl = tf.placeholder(tf.float32, shape=[None, input_shape])\n",
        "        labels_pl = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
        "\n",
        "        # define a variable global_steps\n",
        "        global_steps = tf.Variable(0, trainable=False)\n",
        "\n",
        "        # build a graph that calculate the logits prediction from model\n",
        "        logits = alexnet(images_pl, num_classes, keep_prob)\n",
        "\n",
        "        loss, acc, _ = calc_loss_acc(labels_pl, logits)\n",
        "\n",
        "        # build a graph that trains the model with one batch of example and updates the model params \n",
        "        training_op = train_op(loss, global_steps, base_learning_rate)\n",
        "\n",
        "        # define the model saver\n",
        "        saver = tf.train.Saver(tf.global_variables())\n",
        "        \n",
        "        # define a summary operation \n",
        "        summary_op = tf.summary.merge_all()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            # print(sess.run(tf.trainable_variables()))\n",
        "            # start queue runners\n",
        "            coord = tf.train.Coordinator()\n",
        "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "            train_writter = tf.summary.FileWriter(tb_path, sess.graph)\n",
        "\n",
        "            # start training\n",
        "            for step in range(100):\n",
        "                # get train image / label batch\n",
        "                train_image_batch, train_label_batch = mnist.train.next_batch(batch_size)\n",
        "\n",
        "                train_feed_dict = {images_pl: train_image_batch, labels_pl: train_label_batch}\n",
        "\n",
        "                _, _loss, _acc, _summary_op = sess.run([training_op, loss, acc, summary_op], feed_dict = train_feed_dict)\n",
        "\n",
        "                # store loss and accuracy value\n",
        "                train_loss.append(_loss)\n",
        "                train_acc.append(_acc)\n",
        "                print(\"Iteration \" + str(step) + \", Mini-batch Loss= \" + \"{:.6f}\".format(_loss) + \", Training Accuracy= \" + \"{:.5f}\".format(_acc))\n",
        "\n",
        "                if step % 100 == 0:\n",
        "                    _valid_loss, _valid_acc = [], []\n",
        "                    print('Start validation process')\n",
        "\n",
        "                    for step in range(valid_steps):\n",
        "                        valid_image_batch, valid_label_batch = mnist.test.next_batch(batch_size)\n",
        "\n",
        "                        valid_feed_dict = {images_pl: valid_image_batch, labels_pl: valid_label_batch}\n",
        "\n",
        "                        _loss, _acc = sess.run([loss, acc], feed_dict = valid_feed_dict)\n",
        "\n",
        "                        _valid_loss.append(_loss)\n",
        "                        _valid_acc.append(_acc)\n",
        "\n",
        "                    valid_loss.append(np.mean(_valid_loss))\n",
        "                    valid_acc.append(np.mean(_valid_acc))\n",
        "\n",
        "                    print(\"Iteration {}: Train Loss {:6.3f}, Train Acc {:6.3f}, Val Loss {:6.3f}, Val Acc {:6.3f}\".format(step, train_loss[-1], train_acc[-1], valid_loss[-1], valid_acc[-1]))\n",
        "            \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    train(FLAGS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "Running with Adam Optimizer with learning rate: 0.0001\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "Iteration 0, Mini-batch Loss= 5.380208, Training Accuracy= 0.11719\n",
            "Start validation process\n",
            "Iteration 10: Train Loss  5.380, Train Acc  0.117, Val Loss  8.941, Val Acc  0.102\n",
            "Iteration 1, Mini-batch Loss= 10.148043, Training Accuracy= 0.07812\n",
            "Iteration 2, Mini-batch Loss= 9.948547, Training Accuracy= 0.12500\n",
            "Iteration 3, Mini-batch Loss= 8.298555, Training Accuracy= 0.12500\n",
            "Iteration 4, Mini-batch Loss= 7.718558, Training Accuracy= 0.11719\n",
            "Iteration 5, Mini-batch Loss= 6.344782, Training Accuracy= 0.12500\n",
            "Iteration 6, Mini-batch Loss= 5.385865, Training Accuracy= 0.10156\n",
            "Iteration 7, Mini-batch Loss= 4.580203, Training Accuracy= 0.12500\n",
            "Iteration 8, Mini-batch Loss= 3.539289, Training Accuracy= 0.18750\n",
            "Iteration 9, Mini-batch Loss= 3.088081, Training Accuracy= 0.15625\n",
            "Iteration 10, Mini-batch Loss= 3.035717, Training Accuracy= 0.13281\n",
            "Iteration 11, Mini-batch Loss= 2.972751, Training Accuracy= 0.14062\n",
            "Iteration 12, Mini-batch Loss= 2.888332, Training Accuracy= 0.14062\n",
            "Iteration 13, Mini-batch Loss= 2.650594, Training Accuracy= 0.11719\n",
            "Iteration 14, Mini-batch Loss= 2.534964, Training Accuracy= 0.21094\n",
            "Iteration 15, Mini-batch Loss= 2.500757, Training Accuracy= 0.17188\n",
            "Iteration 16, Mini-batch Loss= 2.732901, Training Accuracy= 0.10156\n",
            "Iteration 17, Mini-batch Loss= 2.563302, Training Accuracy= 0.18750\n",
            "Iteration 18, Mini-batch Loss= 2.638922, Training Accuracy= 0.11719\n",
            "Iteration 19, Mini-batch Loss= 2.464989, Training Accuracy= 0.15625\n",
            "Iteration 20, Mini-batch Loss= 2.429689, Training Accuracy= 0.15625\n",
            "Iteration 21, Mini-batch Loss= 2.210647, Training Accuracy= 0.24219\n",
            "Iteration 22, Mini-batch Loss= 2.165377, Training Accuracy= 0.24219\n",
            "Iteration 23, Mini-batch Loss= 2.303535, Training Accuracy= 0.19531\n",
            "Iteration 24, Mini-batch Loss= 2.081732, Training Accuracy= 0.25781\n",
            "Iteration 25, Mini-batch Loss= 2.044688, Training Accuracy= 0.32031\n",
            "Iteration 26, Mini-batch Loss= 2.002853, Training Accuracy= 0.28125\n",
            "Iteration 27, Mini-batch Loss= 1.955003, Training Accuracy= 0.31250\n",
            "Iteration 28, Mini-batch Loss= 1.997870, Training Accuracy= 0.34375\n",
            "Iteration 29, Mini-batch Loss= 2.057691, Training Accuracy= 0.25781\n",
            "Iteration 30, Mini-batch Loss= 1.782548, Training Accuracy= 0.44531\n",
            "Iteration 31, Mini-batch Loss= 2.021593, Training Accuracy= 0.29688\n",
            "Iteration 32, Mini-batch Loss= 1.715105, Training Accuracy= 0.42188\n",
            "Iteration 33, Mini-batch Loss= 1.656772, Training Accuracy= 0.43750\n",
            "Iteration 34, Mini-batch Loss= 1.596496, Training Accuracy= 0.45312\n",
            "Iteration 35, Mini-batch Loss= 1.416127, Training Accuracy= 0.53125\n",
            "Iteration 36, Mini-batch Loss= 1.365697, Training Accuracy= 0.50781\n",
            "Iteration 37, Mini-batch Loss= 1.583133, Training Accuracy= 0.47656\n",
            "Iteration 38, Mini-batch Loss= 1.638446, Training Accuracy= 0.43750\n",
            "Iteration 39, Mini-batch Loss= 1.266118, Training Accuracy= 0.57812\n",
            "Iteration 40, Mini-batch Loss= 1.294934, Training Accuracy= 0.57031\n",
            "Iteration 41, Mini-batch Loss= 1.293381, Training Accuracy= 0.60938\n",
            "Iteration 42, Mini-batch Loss= 1.387639, Training Accuracy= 0.51562\n",
            "Iteration 43, Mini-batch Loss= 1.132861, Training Accuracy= 0.59375\n",
            "Iteration 44, Mini-batch Loss= 1.072873, Training Accuracy= 0.65625\n",
            "Iteration 45, Mini-batch Loss= 0.910788, Training Accuracy= 0.71875\n",
            "Iteration 46, Mini-batch Loss= 1.160629, Training Accuracy= 0.62500\n",
            "Iteration 47, Mini-batch Loss= 1.124639, Training Accuracy= 0.60156\n",
            "Iteration 48, Mini-batch Loss= 1.150686, Training Accuracy= 0.62500\n",
            "Iteration 49, Mini-batch Loss= 0.928243, Training Accuracy= 0.67188\n",
            "Iteration 50, Mini-batch Loss= 0.746353, Training Accuracy= 0.77344\n",
            "Iteration 51, Mini-batch Loss= 0.871195, Training Accuracy= 0.72656\n",
            "Iteration 52, Mini-batch Loss= 0.900968, Training Accuracy= 0.67969\n",
            "Iteration 53, Mini-batch Loss= 0.741656, Training Accuracy= 0.78125\n",
            "Iteration 54, Mini-batch Loss= 0.762355, Training Accuracy= 0.74219\n",
            "Iteration 55, Mini-batch Loss= 0.793542, Training Accuracy= 0.74219\n",
            "Iteration 56, Mini-batch Loss= 0.741112, Training Accuracy= 0.76562\n",
            "Iteration 57, Mini-batch Loss= 0.585515, Training Accuracy= 0.78125\n",
            "Iteration 58, Mini-batch Loss= 0.522349, Training Accuracy= 0.79688\n",
            "Iteration 59, Mini-batch Loss= 0.677084, Training Accuracy= 0.74219\n",
            "Iteration 60, Mini-batch Loss= 0.726972, Training Accuracy= 0.73438\n",
            "Iteration 61, Mini-batch Loss= 0.495223, Training Accuracy= 0.85938\n",
            "Iteration 62, Mini-batch Loss= 0.730372, Training Accuracy= 0.76562\n",
            "Iteration 63, Mini-batch Loss= 0.555936, Training Accuracy= 0.78125\n",
            "Iteration 64, Mini-batch Loss= 0.389731, Training Accuracy= 0.83594\n",
            "Iteration 65, Mini-batch Loss= 0.464665, Training Accuracy= 0.84375\n",
            "Iteration 66, Mini-batch Loss= 0.570690, Training Accuracy= 0.81250\n",
            "Iteration 67, Mini-batch Loss= 0.470919, Training Accuracy= 0.84375\n",
            "Iteration 68, Mini-batch Loss= 0.474402, Training Accuracy= 0.87500\n",
            "Iteration 69, Mini-batch Loss= 0.518080, Training Accuracy= 0.82812\n",
            "Iteration 70, Mini-batch Loss= 0.532202, Training Accuracy= 0.79688\n",
            "Iteration 71, Mini-batch Loss= 0.432398, Training Accuracy= 0.88281\n",
            "Iteration 72, Mini-batch Loss= 0.417402, Training Accuracy= 0.85938\n",
            "Iteration 73, Mini-batch Loss= 0.508773, Training Accuracy= 0.79688\n",
            "Iteration 74, Mini-batch Loss= 0.589204, Training Accuracy= 0.78125\n",
            "Iteration 75, Mini-batch Loss= 0.397180, Training Accuracy= 0.85938\n",
            "Iteration 76, Mini-batch Loss= 0.687888, Training Accuracy= 0.80469\n",
            "Iteration 77, Mini-batch Loss= 0.543005, Training Accuracy= 0.82031\n",
            "Iteration 78, Mini-batch Loss= 0.418291, Training Accuracy= 0.87500\n",
            "Iteration 79, Mini-batch Loss= 0.477648, Training Accuracy= 0.84375\n",
            "Iteration 80, Mini-batch Loss= 0.421861, Training Accuracy= 0.85938\n",
            "Iteration 81, Mini-batch Loss= 0.411892, Training Accuracy= 0.84375\n",
            "Iteration 82, Mini-batch Loss= 0.527115, Training Accuracy= 0.85156\n",
            "Iteration 83, Mini-batch Loss= 0.507734, Training Accuracy= 0.85156\n",
            "Iteration 84, Mini-batch Loss= 0.405313, Training Accuracy= 0.87500\n",
            "Iteration 85, Mini-batch Loss= 0.433505, Training Accuracy= 0.85156\n",
            "Iteration 86, Mini-batch Loss= 0.377135, Training Accuracy= 0.90625\n",
            "Iteration 87, Mini-batch Loss= 0.334014, Training Accuracy= 0.87500\n",
            "Iteration 88, Mini-batch Loss= 0.284374, Training Accuracy= 0.89062\n",
            "Iteration 89, Mini-batch Loss= 0.321858, Training Accuracy= 0.88281\n",
            "Iteration 90, Mini-batch Loss= 0.407812, Training Accuracy= 0.87500\n",
            "Iteration 91, Mini-batch Loss= 0.232561, Training Accuracy= 0.92969\n",
            "Iteration 92, Mini-batch Loss= 0.295840, Training Accuracy= 0.89062\n",
            "Iteration 93, Mini-batch Loss= 0.436029, Training Accuracy= 0.85938\n",
            "Iteration 94, Mini-batch Loss= 0.404632, Training Accuracy= 0.86719\n",
            "Iteration 95, Mini-batch Loss= 0.285913, Training Accuracy= 0.89844\n",
            "Iteration 96, Mini-batch Loss= 0.362965, Training Accuracy= 0.84375\n",
            "Iteration 97, Mini-batch Loss= 0.314592, Training Accuracy= 0.85938\n",
            "Iteration 98, Mini-batch Loss= 0.294681, Training Accuracy= 0.92969\n",
            "Iteration 99, Mini-batch Loss= 0.456254, Training Accuracy= 0.84375\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}